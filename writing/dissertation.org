#+LaTeX_CLASS: article

#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{lmodern}

#+TITLE: Investigating Dimensionality Reduction Techniques on the LFW Data Set
#+AUTHOR: Martin RP Boissier
#+EMAIL: mrpb201@exeter.ac.uk

* abstract
  \emph{ - M-LDA simple method}
    
* introduction
** face verification
** dimension reduction
   Real-world data such as digital photographs, speech signal
   etc. usually have a high dimensionality. For instance a 250x250
   pixels images can be represented as a 62500 raw-feature
   vector. Though access to an abundance of examples is purely
   beneficial to an algorithm attempting to generalise from the data,
   managing a large number of features is typically of burden to the
   algorithm. Dimensionality reduction techniques were developed to
   address several problems encountered with high dimensional
   data. First one cannot have an intuitive feel for what the data
   looks like in an high dimensional space composed of thousands of
   features, dimension reduction can be used to facilitate data
   representation. The overwhelmingly complex feature sets will also
   slow down most machine learning algorithms and make finding global
   optima difficult. This problem usually referred to as the curse of
   dimensionality is directly related to the difficulty of sampling a
   high dimensional space. The number of training samples needed to
   uniformly cover the feature space is exponentially proportional to
   the number of features. Therefore, many learning algorithms based
   on the distance between samples, such as KNN algorithms and other
   classifier, do not yield good results on high dimensional data.

   The general philosophy sustaining dimensionality techniques is the
   fact that real-life data contain redundancies and noise and that
   the intrinsic dimensionality of the data, defined as the minimum
   number of features needed to account for the observed properties of
   the data. By performing dimensionality reduction, noise can be
   suppressed and redundancies removed while the new low dimensional
   representation of the data still captures most of the information
   needed for clustering, classification or regression algorithms.

* literature overview
  - face recognition
  - Eigen/Fisher-face
  - LFW papers with PCA reduction
  - dimension reduction methods
  - no work on LFW

* LFW

Labeled Faces in the Wild (LFW) is a database of human face images
designed as an aid in studying the problem of unconstrained face
recognition\cite{huang2008labeled}. While, many databases have been developed under
controlled conditions in order to study the influence of specific
parameters such as position, pose, lighting, expression etc., LFW
provides an aid in studying face recognition of photographs spanning a
range of conditions typically encountered by people in their everyday
life.

Although LFW can be used to study a wide range of face recognition
paradigms, it was specifically designed to address the following face
recognition problem : given two pictures, each of which contains a
face, decide whether the two people pictured represent the same
individual. This problem is referred to as the pair matching problem.

The unconstrained pair matching problem can be seen as one of the most
general and fundamental face recognition problem and LFW provides a
database as well as specific experimental paradigms in an effort to
make research performed as consistent and comparable as possible.

The face images were automatically collected from Yahoo! News in
2002-2003, the only constraint being that they were detected by the
Viola-Jones face detector algorithm.

The data set itself contains more than 13,000 target images. Each face
has been labeled with the name of the person pictured. In total 5749
people appear in the images, 1680 of them appear in two or more
images. The remaining 4069 people have just a single image in the
database.

One important aspect that differentiates LFW from other face databases
is that for any given training-testing split, the people in each set
are mutually exclusive. Therefore, one cannot built a model of a face
during training and use it at testing time to artificially improve the
performance of the algorithm. The unconstrained pair matching problem
is meant to focus on the generic problem of differentiating any two
individuals that have never been seen before.

In order to allow fair and accurate comparisons, clear guidelines
concerning the training, validation and testing of pair matching
algorithms are given to prevent unintended over-fitting problems. The
data are organised in two "Views" or subset of the database. View1 is
for algorithm development so that different approaches and parameter
settings can be experimented with without overusing the data, whereas
View2 should be solely used for formal evaluation of the selected
methods and parameters. The first view consists of a training set
composed of 1100 pairs of matched images and 1100 pairs of mismatched
images. The testing set is composed of 500 pairs of matched and 500
pairs of mismatched images. View2 comes with a division in ten
independent subsets of the database. Each subset, or fold, contains
between 527 and 609 different people, and between 1016 and 1783
faces. From all possible pairs, a small set of 300 positive and 300
negative image pairs are provided for each fold. To report accuracy
results on View2, the performance of the classifier is evaluated on 10
separate experiments in a leave-one-out cross validation scheme
obtained by successively concatenating nine of the subsets to form a
training set, with the tenth subset used for testing.

This experimental paradigm is referred to as the restricted settings
as only the given matching and mismatching pairs can be used to
construct the classifier as the name of the persons pictured is not
explicitly given. The unrestricted configuration gives the
experimenter access to the actual names of the people (class labels)
so that as many same/not-same pairs should be constructed. This paper
solely focuses on the restricted configuration.

As far image representation are concerned, we used SIFT descriptors
computed at fixed facial key-points (corners of the mouth, eyes, and
nose) which lead to a 3456 dimensional face descriptor. These data are
available from Guillaumin \cite{guillaumin2009you} and were also used
by Ying \cite{ying2012distance}, therefore focusing on SIFT features
allows a direct comparisons with their methods.


* Dimensionality Reduction

  The problem of dimensionality reduction given a data set represented
  in a $n \times D$ matrix $\mathbf{X}$ consisting of $n$ data-vectors
  $\{\mathbf{x}_1,\dots,\mathbf{x}_n\} \in R^D$ can be defined as
  follows. By assuming that this data-set has intrinsic dimensionality
  $d$ (where $d < D$) which means that the points in data-set
  $\mathbf{X}$ are lying on or near a manifold of dimensionality $d$
  embedded in the D-dimensional space, dimensionality reduction
  techniques transform data-set $\mathbf{X}$ into a new data-set
  $\mathbf{Y}$ with dimensionality $d$.

  In the remainder of this paper we denote the low-dimensional
  counterpart of the data point $\mathbf{x}_i$ by $\mathbf{y}_i$.

  One important aspect and underlying assumption of linear
  dimensionality reduction techniques is that the data set
  $\mathbf{X}$ lies on a linear subspace. Usually, this assumption is
  not as strong for non-linear techniques which usually rely on local
  linearity.



** Linear mapping
  
*** PCA

    Principal Component Analysis is a linear technique which means that
    dimensionality reduction is performed by embedding the data into a
    linear subspace of lower dimensionality.

    The linear subspace is constructed such as the variance of the
    low-dimensional representation of the data is maximised. The
    motivation for performing PCA is often the assumption that
    directions of high variance will contain more information that
    directions of low variance. The rationale behind this could be that
    the noise can be assumed to be uniformly spread. Thus, directions
    of high variance will have a higher signal-to-noise ratio.

    Formally speaking, PCA attempts to find the linear mapping
    orthonormal matrix $\mathbf{M}$ that minimises the cost function
    $trace\ (\mathbf{M}^T \mathbf{S_T}\mathbf{M})$, where
    $\mathbf{S_T}$ is the total scatter matrix or the sample
    covariance matrix of the mean-centred data $\mathbf{X}$.

    \begin{align}
    \mathbf{S_T} &= \sum_{k=1}^N (\mathbf{x_k} - \boldsymbol{\mu}) \cdot (\mathbf{x_k} - \boldsymbol{\mu}) ^T\\
    \mathbf{M} &= \operatorname*{arg\,max}_{\mathbf{M} \in SO(D)} trace(\mathbf{M}^T \mathbf{S_T}\mathbf{M}) \label{eq:pca}
    \end{align}

    
    Where $\boldsymbol{\mu}$ represent the mean vector of
    $\mathbf{X}$. Using Lagrangian multipliers
    \cite{bie2005eigenproblems}, it can be shown that Equation
    \ref{eq:pca} is equivalent to solving the following eigenproblem :
    
    \begin{equation}
    \mathbf{S_T}\mathbf{M}=\lambda\mathbf{M}
    \end{equation}

    The eigenproblem is solved for the $d$ principal eigenvalues
    $\lambda$ by computing the $d$ principal eigenvectors of the
    sample covariance matrix of the zero-mean data $\mathbf{S_T}$. The
    low-dimensional data representations $\mathbf{y}_i$ of the data
    points $\mathbf{x}_i$ are computed by mapping them onto the linear
    basis $\mathbf{M}$, i.e., $\mathbf{Y} = (\mathbf{M}_{D \times
    d})^T \mathbf{X}$. PCA is most useful in the case when data lies
    on or close to a linear subspace of the data set. Given this type
    of data, PCA will find a basis for the linear subspace and allow
    one to disregard the irrelevant features.

*** LDA and M-LDA
    
**** Linear discriminant Analysis (LDA)
    When the learning set is labeled, it makes sense to use this
    information to build a more reliable method for reducing the
    dimensionality of the feature space. Linear Discriminant Analysis
    (LDA) -also known as Fisher's linear discriminant (FLD)- is
    closely related to PCA in that they both look for linear
    combinations of variables which best explain the data. LDA
    explicitly attempts to model the difference between the classes of
    data. PCA on the other hand does not take into account any
    difference in class, a drawback of this approach it that the
    scatter being maximised is due not only to the between-class
    scatter that is useful for classification, but also to the
    within-class scatter that, for classification purposes, is
    unwanted information. LDA selects $\mathbf{M}$ in such a way that
    the ratio of the between-class scatter and the within-class
    scatter is maximised, in other words LDA searches for the project
    axes on which the data points of different classes are far from
    each other while requiring data points of the same class to be
    close to each other. This difference can lead PCA to badly cluster
    the points in the projected space, or worse, to smear the classes
    together as exemplified in Figure \ref{fig:pca} where although PCA
    achieves larger total scatter, LDA achieves greater between-class
    scatter, and , consequently, classification is simplified.

    #+CAPTION: A comparison of PCA and FLD for a two class problem where data for each class lies near a linear subspace. Image from \cite{belhumeur1997eigenfaces}.
    #+ATTR_LaTeX: scale=0.2
    #+LABEL: fig:pca
    [[./pca-lda.png]]

    For all samples of all classes the between-class scatter matrix
    $\mathbf{S_{B}}$ and the within-class scatter matrix $\mathbf{S_{W}}$ are defined
    by:


    \begin{align}
    \mathbf{S_T} &= \mathbf{S_B} + \mathbf{S_W}\\
    \mathbf{S_{B}} &= \sum_{i = 1}^c N_i (\mathbf{x}_i - \mathbf{\boldsymbol{\mu}}) \cdot (\mathbf{x}_i - \mathbf{\boldsymbol{\mu}})^T \\
    \mathbf{S_W} &= \sum_{i = 1}^c \sum_{\mathbf{x}_k \in C_i} (\mathbf{x}_i - \mathbf{\boldsymbol{\mu}}_i) \cdot (\mathbf{x}_i - \mathbf{\boldsymbol{\mu}}_i)^T
    \end{align}


    where $N_i$ is the number of training samples in class $i$, $c$ is
    the number of distinct classes, $\boldsymbol{\mu}_i$ is the mean
    vector of samples belonging to class $i$ and $C_i$ represents the
    set of samples belonging to class $i$ with $\mathbf{x_k}$ being
    the $k$-th vector of that class. $\mathbf{S_W}$ represents the
    scatter of features around the mean of each class and
    $\mathbf{S_B}$ represents the scatter of features around the
    overall mean for all classes. As mentioned, the goal is to
    maximise $\mathbf{S_B}$ while minimising $\mathbf{S_W}$, and
    therefore to find the orthonormal projection $\mathbf{M}$ that
    maximises the following ratio.

    \begin{equation}
    \mathbf{M} = \operatorname*{arg\,max}_{\mathbf{M} \in SO(D)} \frac{trace(\mathbf{M}^T \mathbf{S_B}\mathbf{M})}{trace(\mathbf{M}^T \mathbf{S_W}\mathbf{M})}
    \end{equation}

    This time the cost function can be assimilated to a generalised
    eigenvalue problem, with both $S_B$ and $S_W$ symmetric and
    positive semi-definite.

    \begin{equation}
    \mathbf{S_B} \mathbf{M} = \lambda \mathbf{S_W} \mathbf{M} \label{eq:lda}
    \end{equation}

    Also the maximum rank of $\mathbf{S_B}$ in this formulation is
    $c - 1$ \cite{shylajadimensionality}. Thus LDA cannot produce more
    than $c - 1$ features. Furthermore in the case that the number of
    training samples is much smaller that the number of features, the
    rank of $\mathbf{S_W}$ is at most $N - c$, therefore in order to
    avoid the complication of singular matrices, the training set is
    usually first projected onto an $N - c$ orthogonal subspace with
    PCA before applying LDA \cite{belhumeur1997eigenfaces}.

**** Matching-LDA    

    LDA cannot be applied as such under the restricted configuration
    of the LFW database as the name of the people pictured is not
    given. In effect, the class labels are needed to compute the mean
    vector of each class but cannot be inferred from this
    paradigm. Nonetheless similarity and dissimilarity pairs
    constitute partial knowledge of the classes as a matching pair is
    made of two images picturing the same individual and therefore
    belonging to the same class label, in a similar fashion, a
    dissimilarity pair indicates that the individuals belong to two
    different classes. Similarly to LDA, we would like to find
    $\mathbf{M}$ such as that the projections $\mathbf{y_i},
    \mathbf{y_j}$ of the data vectors $\mathbf{x_i}, \mathbf{x_j}$
    would be close to each other under the metric associated with the
    projection space when $\mathbf{x_i}$ and $\mathbf{x_j}$ constitute
    a matching pair, conversely the distance between $\mathbf{y_i}$
    and $\mathbf{y_j}$ should be "greater" when $\mathbf{x_i}$ and
    $\mathbf{x_j}$ form a dissimilarity pair. We propose a new
    supervised linear dimension reduction method closely related to
    LDA and adapted to the pair matching problem : Matching-LDA
    (M-LDA). M-LDA defines the aforementioned similarity pair and
    dissimilarity pair scatter matrices as such :
    
    \begin{align}
    \mathbf{S_{SP}} &= \sum_{(i, j) \in SP} (\mathbf{x}_i - \mathbf{x}_j) \cdot (\mathbf{x}_i - \mathbf{x}_j)^T\\
    \mathbf{S_{DP}} &= \sum_{(i, j) \in DP} (\mathbf{x}_i - \mathbf{x}_j) \cdot (\mathbf{x}_i - \mathbf{x}_j)^T
    \end{align}

    where $SP$ contains the indices of similarity pairs and $SP$ of
    dissimilarity pairs respectively. The associated definition of
    $\mathbf{M}$ and its related eigenvalue problem under M-LDA are as
    follows :
    
    \begin{align}
    \mathbf{M} &= \operatorname*{arg\,max}_{\mathbf{M} \in SO(D)} \frac{trace(\mathbf{M}^T \mathbf{S_{DP}}\mathbf{M})}{trace(\mathbf{M}^T \mathbf{S_{SP}}\mathbf{M})}\\
    \mathbf{S_{DP}} \mathbf{M} &= \lambda \mathbf{S_{SP}} \mathbf{M}
    \end{align}

** Manifold Learning

   Linear dimensionality reduction methods, despite their popularity,
   also have a number of limitations. Perhaps the most blatant
   drawback is the requirement that the data lie on linear
   subspace. What if the plane was curled as it is in Figure
   \ref{fig:swiss-roll}? Though the data is still intuitively
   two-dimensional, PCA, LDA and other linear methods, will not
   correctly extract this two-dimensional structure. In mathematical
   term, the swiss-roll structure is called a manifold. A manifold is
   a topological space that is locally Euclidean, therefore, the
   swiss-roll is considered to be a two-dimensional manifold because
   it locally "looks like" a copy of $\mathbb{R}^2$. Manifold learning
   algorithms essentially attempt to duplicate the behaviour of PCA,
   but on manifolds instead of linear subspaces. The two manifold
   learning algorithms presented, Isomap and LLE, requires a
   neighbourhood-size parameter $k$ corresponding to the number of
   samples neighbouring a given data point $\mathbf{x}_i$. As
   previously mentioned, it is important to note that usually manifold
   learning algorithms assume that within each neighbourhood the
   manifold is approximately flat.

   #+CAPTION: A curled plane: the swiss roll
   #+ATTR_LaTeX: scale=0.2
   #+LABEL: fig:swiss-roll
    [[./swiss-roll.png]]

    
*** ISOMAP

     If the high-dimensional data lies on a near a curved manifold,
     the Euclidean distance in the input space may not accurately
     reflect the intrinsic similarity of two arbitrary points. This
     problem is manifest for the Swiss roll data set where the
     geodesic distance (distance along a manifold) is much larger that
     the typical inter-point distance.

     Isomap -short for isometric feature mapping- was one of the first
     algorithms introduced for manifold learning. It may be viewed as
     an extension to Multidimensional Scaling (MDS), a classical
     method for embedding dissimilarity information into Euclidean
     space. Isomap consists of two main steps:
     
	1. Estimate the geodesic distances between points in the input
           using shortest-path distances on the data set's $k$-nearest
           neighbour graph.
	2. Use MDS to find points in low-dimensional Euclidean space
           whose interpoint distances match the distances found in
           step 1.

    Isomap attempts to preserve pairwise geodesic distances between
    data points. By assuming that the manifold is smooth enough
    between nearby points and locally linear, the Euclidean distance
    between nearby points in the high-dimensional data space is
    assumed to be a good approximation to the geodesic distances
    between these points. This approximation breaks down as the
    distance between points increases. Thus, to perform that
    estimation, the Isomap algorithm first constructs $G$ the
    $k$-nearest neighbour graph that is weighted by the Euclidean
    distances in which every data point $\mathbf{x}_i$ is connected
    with its $k$ nearest neighbours $\mathbf{x}_{i_j}\
    j\in\{1,\dots,k\}$ in the data set $\mathbf{X}$. Then, the
    algorithm runs a shortest-path algorithm (such as Dijkstra's or
    Floyd's) and uses its output as the estimates for the remainder of
    the geodesic distances.

    Once these geodesic distances are calculated, the Isomap algorithm
    finds points whose Euclidean distances equal these geodesic
    distances. Multidimensional Scaling (MDS) is a classical technique
    that may be used to find such points. MDS finds the rank d
    projection that best preserves the inter-point distance matrix
    $\mathbf{D}$ whose entries represent the Euclidean distance
    between high-dimensional data points or the computed geodesic
    distances in the present case.

    Classical MDS finds the linear mapping $\mathbf{Y}$ that minimises
    the cost function

    \begin{equation}
    \Phi(\mathbf{Y}) = \sum_{ij}d_{ij}^2 - \|\mathbf{y}_i - \mathbf{y}_j\|^2
    \end{equation}

    It can be shown that the minimum of this cost function is given by
    the eigen-decomposition of the Gram matrix $\mathbf{B}$ which
    entries could be obtained by double-centring the pairwise geodesic
    distance matrix $\mathbf{D}$ \cite{cayton2005algorithms}.

    \begin{align}
    \mathbf{B} &= -\frac{1}{2} \mathbf{H} \mathbf{D} \mathbf{H},\ \mathbf{H} = \mathbf{I} - \frac{1}{n}\mathbf{11}^T\\
    b_{ij} &= -\frac{1}{2} \left(d_{ij}^2 - \frac{1}{n}\sum_{l}d_{il}^2 - \frac{1}{n}\sum_{l}d_{lj}^2 + \frac{1}{n^2}\sum_{lm}d_{lm}^2 \right)\\
    \end{align}

    In that case, the mapping $\mathbf{Y}$ is obtained by
    eigendecomposition of the Gram matrix $\mathbf{B}$, the top $d$
    eigenvectors representing the coordinates of this Euclidean space.

    \begin{equation}
    \mathbf{Y} = (\boldsymbol{\Lambda}_{d \times d}) ^{\frac{1}{2}} (\mathbf{V}_{n \times d}) ^T \label{eq:isomap}
    \end{equation}

    where $\mathbf{V}$ and $\boldsymbol{\Lambda}$ are the eigenvectors
    and eigenvalues of $\mathbf{B}$ respectively. Therefore, in order
    to obtain the final low dimensional representations $\mathbf{y}_i$
    of the data points $\mathbf{x}_i$, MDS is performed on the Gram
    matrix $\mathbf{B}$ computed using an estimation of the interpoint
    geodesic distance matrix $\mathbf{D}$.
    

*** LLE

     In contrast to Isomap, Local Linear Embedding (LLE) attempts to
     solely preserve local properties of the data. The manifold is
     visualised as a collection of overlapping coordinate patches and
     if the neighbourhood sizes are small and the manifold is
     sufficiently smooth, then these patches will be approximately
     linear. The local properties of the data manifold are constructed
     by writing the high-dimensional data points $\mathbf{x}_i$ as
     linear combination $\mathbf{w}_i$ of its $k$ nearest neighbours
     $\mathbf{x}_{i_j}\ j\in\{1,\dots,k\}$. The weight matrix
     $\mathbf{W}$ can be obtained by minimising
     
     \begin{equation}
     \Phi(\mathbf{W}) = \sum_{i=1}^n \|\mathbf{x}_i - \sum_{j=1}^n w_{ij}\mathbf{x}_{j}\|^2 
     \end{equation}

     under the conditions $\sum_{j=1}^n w_{ij} = 1$ for any data point
     $\mathbf{x}_i$ and $w_{ij}=0$ if $\mathbf{x}_j$ is not a
     neighbour of $\mathbf{x}_i$. The first constraint reflects that
     each point is represented as a convex combination of its
     neighbours and that the weights are invariant to global linear
     transformations , while the second assures that LLE is a local
     method \cite{cayton2005algorithms}. Under those couple of
     constraints on the weights, the local linearity assumption
     implies that the reconstruction weights are invariant to
     translation, rotation, and rescaling. Because of the invariance
     to these transformations, any linear mapping of the hyperplane to
     a space of lower dimensionality preserves the reconstruction
     weights in the space of lower dimensionality. In other words, if
     the low-dimensional data representation preserves the local
     geometry of the manifold, the reconstruction weights
     $\mathbf{w}_i$ that reconstruct datapoint $\mathbf{x}_i$ from its
     neighbours in the high-dimensional data representation also
     reconstruct datapoint $\mathbf{y}_i$ from its neighbours in the
     low-dimensional data representation. Therefore, in the
     low-dimensional representation of the data, LLE attempts to
     retain the reconstruction weights in the linear combinations as
     good as possible. As consequence, finding the d-dimensional data
     representation $\mathbf{Y}$ amounts to minimising the cost
     function in which, this time, $\mathbf{W}$ is fixed

     \begin{equation}
     \Phi'(\mathbf{Y}) = \sum_{i=1}^n \|\mathbf{y}_i - \sum_{j=1}^n w_{ij}\mathbf{y}_{j}\|^2 \label{eq:lle}
     \end{equation} 

     There are also a couple of constraints on $\mathbf{Y}$
     \cite{van2007dimensionality}. First, $\mathbf{Y}^T \mathbf{Y} =
     \mathbf{I}$, which forces the solution to be of rank $d$ and to
     exclude the trivial solution $\mathbf{Y} = \mathbf{0}$. Second,
     $\sum_i \mathbf{Y}_i = \mathbf{0}$; this constraint centres the
     embedding on the origin. The cost function \ref{eq:lle} may also
     be rewritten as

     \begin{equation}
     \mathbf{Y} &= \operatorname*{arg\,min}_{\mathbf{Y}} trace(\mathbf{Y}^T \mathbf{M}\mathbf{Y})
     \end{equation}

     where
     
     \begin{equation}
     m_{ij} =  \delta_{ij} - w_{ij} - w_{ji} + \sum_k w_{ki} w_{kj}
     \end{equation}

     and $\delta_{ij} = 1$ if $i=j$ and 0 otherwise.  As shown in
     Equation \ref{eq:pca}, this problem is equivalent to computing
     the eigenvectors, which this time, corresponds to the smallest
     $d$ nonzero eigenvalues of the matrix $\mathbf{M}$ which can also
     be rewritten as the inproduct $(\mathbf{I} -
     \mathbf{W})^T(\mathbf{I} - \mathbf{W})$
     \cite{van2007dimensionality}. The eigenproblem equivalence is
     given by

     \begin{equation}
     \mathbf{M} \mathbf{Y} = \lambda \mathbf{Y} \label{eq:lle}
     \end{equation}


*** k-neighbourhood graph/adaptative algorithm

    Neighbourhood definition is the most important step in all
    bottom-up approaches for data embedding such as Isomap and
    LLE. The shape of the manifold is in most cases unknown but a
    common assumption is that in small patches the surface is smooth,
    and that close neighbours of a data point likely lie on the same
    part of the manifold and have a similar orientation. Therefore,
    properties of the locality at each data point are commonly
    estimated using its nearest neighbours. Two formulations are
    commonly used: a fixed number of neighbours ($k$-nearest
    neighbours), or all neighbours within a fixed radius $\epsilon$
    (hyper-sphere). The $k$-nearest neighbours version is more common
    since the sparseness of the resulting structures is guaranteed and
    Efficient versions exist of the Dijkstra algorithm (used in
    Isomap) that take advantage of the sparseness of the input graph
    \cite{mekuz2006parameterless}. On the other hand, if an
    hypersphere is used, it is difficult to predict if a selected
    radius will include any neighbours at all at every point.

    Two related problems emerges with these methods. First, the choice
    of parameter typically has a dramatic effect on the
    transformation. If the neighbourhoods are too small, disconnected
    clusters tend to form. The manifold is mapped in this case as a
    set of disjoint components and the global structure is lost. Since
    LLE performs a set of local optimisations, it is highly dependent
    on links created by sufficiently large neighbourhoods to discern
    global structure. On the other hand, setting the neighbourhood to a
    size that is too large creates links to parts of the manifold that
    are geodesically far. Isomap is especially sensitive to this
    problem since the shortest paths algorithm will tend to drain
    multiple paths through such shortcuts, affecting distance
    estimates globally. However, with small neighbourhood sizes, the
    computed graph geodesic greatly overestimates the true geodesic
    distances in linear surfaces. The second related drawback is that
    those methods do not guarantee that the transitive closure of
    neighbours of a data point includes all data points. If the
    neighbourhoods do not overlap with each other, LLE and Isomap may
    fail to embed all data points into a single global coordinate
    system. For this reason, graph-based methods require that the data
    are uniformly distributed and well-sampled. In many applications,
    however, the data set has limited number of records or is unevenly
    sampled.

    In \cite{mekuz2006parameterless}, the authors describe a strategy
    for selecting a neighbourhood size adaptively that does not require
    any parameters, based on estimates of intrinsic dimensionality and
    tangent orientation. Several algorithms making use of
    spanning-trees and addressing the problem of disconnected
    components while constructing the neighbourhood graph are
    presented in \cite{yang2006building}. So far,
    adaptative-neighbourhood methods do not guarantee that the
    constructed neighbourhood graph should be connected, and
    conversely, the construction of connected graphs are not
    parameterless methods and still requires a $k$
    parameter. Combining these techniques escapes the scope of the
    present research and therefore, besides applying the more
    traditional approach that consists of simply running an algorithm
    over a variety of choices of neighbourhood size and comparing the
    outputs to pick the appropriate $k$, we also investigated the use
    of an adaptative algorithm. The problem of disconnected neighbour
    graphs and the following data embedding was addressed using the
    out-of-sample extension as described in the next section.

*** out-of-sample extension

    The two linear dimensionality reduction techniques presented
    differ from the manifold learning ones in terms of data
    embedding. Whereas PCA and LDA give a linear mapping $\mathbf{M}$
    to project the high dimensional data set $\mathbf{X}$ into its low
    dimension counterpart $\mathbf{Y}$ as showed in Equation
    \ref{eq:pca} and \ref{eq:lda}, this is not the case for Isomap nor
    LLE, which do not provide any mapping $\mathbf{M}$ from the high
    dimensional space to the low dimensional space but directly
    compute the data embedding $\mathbf{Y}$ by solving the
    eigenproblems from Equation \ref{eq:isomap} and \ref{eq:lle}.

    This problem was already mentioned in
    \cite{shylajadimensionality}, which states that although Isomap,
    LLE and other nonlinear methods do yield impressive results on
    some artificial dataset-based benchmarks, they yield maps that are
    defined only on the training data points and how to evaluate the
    maps on novel test data points remains unclear.

    Isomap and LLE can be described as non-parametric dimensionality
    reduction methods \cite{van2007dimensionality}. This means that
    those techniques do no specify a direct mapping from the
    high-dimensional to the low-dimensional space. The non-parametric
    nature of those algorithms is a disadvantage as it is not possible
    to generalise to held-out or new test data without performing the
    dimensionality reduction technique again which usually consists of
    recomputing eigenvectors. As far as learning algorithms are
    concerned, the training and testing usually being two distinct
    phases, non-parametric dimension reduction method could not be
    used without performing unwanted data overfittings. Fortunately,
    most unsupervised learning algorithms based on an
    eigendecomposition can be seen as more generally learning
    eigenfunctions of a kernel which stem extensions to the
    out-of-sample problem. To obtain an embedding for a new data
    points, \cite{bengio2004learning} propose to use the Nystrom
    formula. Given the embedding $\mathbf{Y}$ from the data set
    $\mathbf{X}$, it has been proved that the eigenvectors and
    eigenvalues computed from the associated eigenproblem, converge as
    more and more sample points are added to $\mathbf{X}$. Each
    eigenvector converges to an eigenfunction. Therefore, manifold
    learning methods based on an eigendecompostion problem can be seen
    as special cases of a more general learning problem, that of
    learning the principal eigenfunctions of defined from a specific
    kernel. \cite{bengio2004out} provides a general framework in which
    Isomap and LLE are represented by a kernel function which gives
    rise to the matrices constituting the eigenproblem to solve.

    The use that was made of the out-of-sample extension is
    twofold. Data points from the testing set were embedded by
    applying the out-of-sample extension obtained from the
    low-dimensional projection of biggest connected component of
    training set. Similarly, the training data points, that could not
    have been embedded due to the fact that they were not part of the
    main connected component of the $k$-neighbourhood graph, were also
    projected to the low-dimensional space using the out-of-sample
    extension.


** General Properties

   Many of the techniques presented are highly interrelated, and in
   certain cases equivalent. First PCA is identical to performing MDS
   when the dissimilarity matrix is a Euclidean distance matrix due to
   the relation between the eigenvectors of the covariance matrix and
   the doublecentred squared Euclidean distance matrix. Secondly,
   performing MDS on a pairwise geodesic distance matrix is identical
   to performing Isomap. Furthermore, as it well be seen when
   considering the out-of-sample extension, these techniques can also
   be viewed upon as special cases of the more general problem of
   learning eigenfunctions.

   | Technique | Computational                | Memory                           |
   |-----------+------------------------------+----------------------------------|
   | PCA       | $O(D^3)$                     | $O(D^2)$                         |
   | LDA       | $O(min(n,D)nD + min(n,D)^2)$ | $O(nD + min(n,D)D +  min(D,n)n)$ |
   | Isomap    | $O(n^3)$                     | $O(n^2)$                         |
   | LLE       | $O(pn^2)$                    | $O(pn^2)$                        |



* experiment

    



** view1

*** view1 results / ill-conditioned 

    
    View1 was used to test the implementation of the different linear
    and non-linear dimensionality reduction methods as well as
    evaluate their performance in order to select the best approach to
    the unconstrained pair matching problem.

    The performance of those different dimensionality reduction
    methods was measured by computing their accuracy on the testing
    data set. Following the reduction of the training and testing set,
    the Euclidean distance between each of the two low-dimensional
    vectors constituting a pair was computed and compared to a
    threshold $\theta$. When the distance between the two image
    vectors is inferior to $\theta$ the pair is classified as a
    similarity pair, in the same way, a distance superior to the
    threshold is assimilated to a dissimilarity pair. The value of
    $\theta$ was fixed using the training data set in order to
    minimise similarity and dissimilarity pairs classification, the
    final accuracy was then measured using the same threshold on the
    testing set.

    The following results were obtained by evaluating the accuracy of
    the linear and non-linear techniques, as well as the raw data on
    View1 testing data set :
    
    | method         | raw-features |    PCA |  M-LDA | Isomap |    LLE |
    |----------------+--------------+--------+--------+--------+--------|
    | /              |            < |        |        |        |        |
    | accuracy       |       0.6680 | 0.6910 | 0.5020 | 0.6460 | 0.6650 |
    | dimension      |         3456 |     96 |      2 |     79 |     12 |
    | $k$            |              |        |        |    107 |     12 |
    |----------------+--------------+--------+--------+--------+--------|
    | sqrt-accuracy  |       0.6830 | 0.6900 | 0.5010 | 0.6480 | 0.6490 |
    | sqrt-dimension |         3456 |     85 |      7 |     48 |      2 |
    | $k$            |              |        |        |     93 |     78 |
    |----------------+--------------+--------+--------+--------+--------|
    | time (s)       |           12 |    405 |   3591 |   1314 |   1429 |

    As mentioned earlier, overusing View1 data set is not an issue,
    and the present results were obtained by overfitting the
    data. Variables such as the reduction dimension and the
    $k$-neighbourhood parameter were selected by exhaustive search in
    order to maximise the accuracy on the testing set. It was
    therefore possible to obtain a first approximation of the maximal
    performance of those different approaches to the unconstrained
    pair matching problem.

    Performing PCA on the data set gave a better accuracy than simply
    using the raw SIFT features. Surprisingly, the performance of the
    two non-linear techniques Isomap and LLE was inferior to PCA and
    even to the accuracy measured on the raw features while requiring
    more computational time. The worst performance was given by M-LDA
    achieving an accuracy of 50.2\%, this result indicates that the
    method completely failed to differentiate between the classes in
    the low dimensional space. In effect, an algorithm randomly
    selecting the output of this binary problem would also have
    obtained an overall accuracy of 50% as the number of similarity
    and dissimilarity pairs part of the training and testing sets is
    the same.

    As suggested in \cite{guillaumin2009you} and
    \cite{ying2012distance}, the accuracy was also measured on the
    data set obtained by taking the square root value of the raw
    features. However, no noticeable improvement of the result was
    noticed. 

    The poor performance of M-LDA was investigated and it appeared
    that in its current form the algorithm leads to an ill-conditioned
    problem. When computed on the SIFT data, which dimensionality
    equates 3456, the conditioning of the two matrices
    $\mathbf{S_{SP}}$ and $\mathbf{S_{DP}}$ is extremely high. The
    conditioning number of a matrix $\mathbf{M}$ represents the
    sensitivity of the solution of the linear algebraic system
    $\mathbf{Mx}=\mathbf{b}$ with respect to changes in vector
    $\mathbf{b}$ and in matrix $\mathbf{M}$. Note that this is before
    the effects of round-off error are taken into account;
    conditioning is a property of the matrix, not the algorithm or
    floating point accuracy of the computer used to solve the
    corresponding system. When the condition number is exactly one,
    then the algorithm may find an approximation of the solution with
    an arbitrary precision, the condition number may also be infinite,
    in which case the algorithm will not reliably find a solution to
    the problem, not even a weak approximation of it with any
    reasonable and provable accuracy. When computed on the SIFT
    features, the order of magnitude of the conditioning of
    $\mathbf{S_{DP}}$ and $\mathbf{S_{SP}}$ was about $10^{20}$ which
    explains the poor performance of M-LDA on this data set as the
    generalised eigenproblem could not have been accurately
    solved. Fortunately, it is possible to circumvent the problem
    without modifying the definition of $\mathbf{S_{DP}}$ and
    $\mathbf{S_{SP}}$ by reducing the dimensionality of the data set
    on which the computation is performed. While other approaches
    exist this one was selected for its simplicity. By reducing the
    dimensionality of the data set to 78 using PCA, the conditioning
    of $\mathbf{S_{SP}}$ was reduced to 35.8514. In other words, by
    performing a pre-reduction using PCA on the data set, M-LDA could
    be successfully applied as the method did not present
    ill-conditioned behaviours in the low dimensional space.

    


*** view1 pca-prereduction

    The idea of performing a pre-reduction of the data set using PCA
    was further explored in order to analyse its influence on M-LDA as
    well as on the two manifold learning techniques. In order to
    select the dimensionality of the PCA reduction as well as the
    dimension of the second reduction technique, an exhaustive search
    was once more performed. Nonetheless, the values of the
    $k$-neighbourhood parameter found during the previous experiment
    were conserved for that second experiment in order to maintain a
    reasonable computational time. Furthermore, thanks to the PCA
    pre-reduction and its impact on the complexity of the algorithms
    involved, a computation of the neighbourhood graph using the
    aforementioned adaptative method became possible. This approach
    was tested for Isomap only.


    | method           |   M-LDA |  Isomap | Isomap-adaptative |     LLE |
    |------------------+---------+---------+-------------------+---------|
    | /                |       < |         |                   |         |
    | accuracy         | 0.81000 | 0.65800 |           0.65100 | 0.68100 |
    | pca dimension    |      78 |      50 |                20 |      58 |
    | method dimension |      25 |      42 |                17 |      35 |
    |------------------+---------+---------+-------------------+---------|
    | sqrt-acc         | 0.81000 | 0.65500 |            0.6580 | 0.65400 |
    | pca dimension    |      62 |      55 |                33 |      51 |
    | method dimension |      29 |      44 |                33 |      29 |
    |------------------+---------+---------+-------------------+---------|
    | time (s)         |       2 |     357 |              2583 |    1043 |

    Performing a PCA pre-reduction proved a success as the accuracy of
    every reduction methods was improved. It appears that performing a
    PCA pre-reduction followed by an Isomap reduction takes less
    computational time than directly computing the reduction on the
    raw features while providing a better accuracy. The adaptative
    method yields similar results in terms of accuracy and its
    computational time remains inferior to what would have been
    obtained by testing a wide range of values of $k$. Nonetheless,
    one must acknowledge that despite those improvements, both Isomap
    and LLE failed to perform better than a simple PCA reduction which
    already provided an accuracy of 69.1\%. The performance of M-LDA
    was greatly improved by the fact that it was that not subject to
    ill-conditioning anymore. With an accuracy of 81\% and less than
    two seconds to perform the second reduction, M-LDA proved an
    excellent linear surpervised dimensionality reduction method. Once
    more, no further improvement of the accuracy relatively to the use
    of the square root of the SIFT features was noticed.

** view2

   The different experiments run on View1, the part of the LFW
   database dedicated to algorithm development, permitted to select
   the model to was to be formally assesed on View2. Isomap and LLE
   proved to be inferior in terms of accuracy to PCA while requiring a
   substantial computational time and were therefore discarded. Only
   the performance of PCA, M-LDA as well as the SIFT features were
   evaluated using View2 10 cross-validation scheme. Two approaches
   were considered to fix the dimension parameters for both PCA and
   M-LDA: to maintain the different values that were used when
   evaluating the performance on View1, or to fix those parameters by
   performing a cross-validation on the training data set of
   View2. This second approach was facilitated by the fact that the
   training set was already composed of 9 mutually exclusive folds of
   training samples. Contrary to View1 training data set which would
   have been difficult to subdivise in independent partitions, it was
   easy to use View2 folds in a 9 cross-validation scheme to fix the
   PCA and M-LDA reduction dimensionality. Parameters providing the
   greatest mean accuracy over the 9 cross-validation on the training
   set were selected to run the algorithm on the remaining testing
   fold. As the square root values of the SIFT features did not
   present any accuracy improvements on View1, it was decided not to
   evaluate their performance on View2.


   | method       | accuracy            | cross validation      |
   |--------------+---------------------+-----------------------|
   | /            | <                   | <                     |
   | raw-features | 0.6755 $\pm$ 0.0058 | -                     |
   | PCA          | 0.6808 $\pm$ 0.0053 | 0.6837 $\pm$ 0.0056   |
   | M-LDA        | 0.7648 $\pm$ 0.0057 | *0.7998 $\pm$ 0.0055* |

   The difference in terms of accuracy between the two parameter
   selection approaches is related to the fact that the parameters
   from View1 were selected by overfitting the data to obtain the
   highest accuracy on the testing data set, and therefore, were not
   likely to produce as good results on View2. This explain why an
   accuracy of 76 and not 81\%, as on View1, is
   obtained. Cross-validation on View2 to fix the paramaters proved to
   be almost as good as overfitting the data set on View1. One
   plausible explanation could be related to the number of training
   pairs: View1 provided a training set composed of 2200 pairs,
   whereas View2 9 cross-validation parameter selection scheme
   consisted of 5400 training pairs (8 folds). Generally, the higher
   the training sample number, the better the accuracy.
   
   M-LDA was also compared to two state-of-the-art distance metric
   learning algorithms which also rely on SIFT descriptors and
   therefore constitute a fair ground for comparison. Our newly
   developed method proves to be as good as LDML,
   \cite{guillaumin2009you}, and DML-eig \cite{ying2012distance}
   published in 2009 and 2012 respectively.


   | method  | accuracy              |
   |---------+-----------------------|
   | /       | <                     |
   | LDML    | 0.7927 $\pm$ 0.006    |
   | DML-eig | 0.8127 $\pm$ 0.0230   |
   | *M-LDA* | *0.7998 $\pm$ 0.0055* |

   A diagram picturing the ROC curbs of the different methods is
   present below.
   
   [[file:../experiment/roc/lfw_restricted_roc_curve.png]]

   

* conclusion
* acknowledgements
* bibliography

\bibliography{papers} \bibliographystyle{plain}
