* Dimensionality Reduction Methods

** Principal Component Analysis
*** Type of method
    PCA is a linear unsupervised global dimensionality reduction method

*** Principles
    PCA can be mathematically defined as an orthogonal linear
    transformation that transforms the data to a new coordinate system
    such that the greatest variance by any projection of the data
    comes to lie on the first coordinate, the second greatest variance
    on the second coordinate, and so on. The set of observations is
    projected into a subspace of linearly uncorrelated variables
    called principal components. The new orthonormal basis of
    principal components is constructed to maximise the variance of
    the data-set in a way that the projection of the data on the first
    principal component has the largest possible variance, and each
    succeeding component in turn has the highest variance possible
    under the constraint that it be orthogonal to the preceding
    components.  Equivalently, it can be defined as the linear
    projection that minimises the average projection cost, defined as
    the mean squared distance between the data points and their
    projections.
    

*** Applications
    This method can be though of as revealing the internal structure
    of the data in a way which best explains the variance in the
    data. Thus, PCA can be used for dimensionality reduction, data
    visualisation by projecting on two or three principal components,
    lossy data compression, feature extraction

*** Hypotheses
    PCA is sensitive to the relative scaling of the original
    variables. Therefore, the data should be mean centred as well as
    standardised in order to scale the data. However, even if
    multivariate normality is supposed, this is not a critical
    assumption. Better results are obtained for continuous and
    normally distributed data

** Linear Determinant Analysis
*** Type of Method
    LDA is a linear supervised global dimensionality reduction mostly used for classification
*** Principles
    LDA is closely related to principal component analysis in that
    they both look for linear combinations of variables which best
    explain the data. However, whereas PCA rely solely on maximising
    the total variance, LDA makes use of the labelled data-set to
    control the difference between inner-class variance and
    between-class variance and attempt to minimise the former and
    maximise the latter. Therefore LDA seeks to reduce dimensionality
    while preserving as much of the class discriminatory information
    as possible
*** Applications
    LDA is used to reduce the number of features to a more manageable number before classification. 
*** Hypotheses
    Usually the classes are assumed to be normally distributed
    (unimodal Gaussian likelihoods), if the distributions are
    significantly non-Gaussian, the LDA projections may not preserve
    complex structure in the data needed for classification.  LDA also
    produces at most C - 1 future projections where C is not number
    of classes composing the data.  LDA will also fail if
    discriminatory information is not in the mean but in the variance
    of the data
    
** ISOMap
*** Type of Method
    ISOMap is a non-linear unsupervised global distance metric learning method
*** Principles
    ISOMap is a manifold learning technique aimed at identifying the
    geodesic distance along the low-dimensional manifold. In effect,
    points far part on the underlying manifold, as measured by their
    geodesic distances, may appear deceptively close in the
    high-dimensional input space, as measured by their straight-line
    Euclidean distance. ISOMap extends metric multidimensional scaling
    by incorporating an approximation of the geodesic distances by
    constructing a weighted graph based on a proximity measure such as
    nearest k Euclidean neighbours in the high-dimensional space. Once
    the graph as been contracted, the remaining distances are
    approximated using a shortest-path algorithm, the new-found
    distance matrix is then used conjointly with MDS in order to
    exhibit the low-dimensional embedding.
*** Applications
    As many non-linear unsupervised methods, ISOMap can be seen as a
    distance metric learning process but also as a dimensionality
    reduction method
*** Hypotheses

** Multidimensional Scaling
*** Type of method
    As PCA, MDS is a linear unsupervised global dimensionality reduction method

*** Principles
    MDS find a projection that best preserves the inter-point distance (dissimilarity) given by the pairwise distance matrix of the data-set.

*** Applications
    MDS is usually used for information visualisation by representing the data in a two and three dimensional space. 

*** Hypotheses
    Only the distance matrix is needed for Classical MDS



** Laplacian Eigenmaps
*** Type of Method
*** Principles
*** Applications
*** Hypotheses

** Locally Linear Embedding
*** Type of Method
*** Principles
*** Applications
*** TODO Hypotheses


* <2012-07-03 Tue 11:07>
** Tasks
*** DONE review initial papers
    CLOSED: [2012-07-03 Tue 15:49]
*** DONE select new papers to study
    CLOSED: [2012-07-03 Tue 19:37]
*** TODO implement algorithms

** review
*** Labeled Faces in the Wild   
    [[file:papers/labeled_faces_in_the_wild.pdf][file:~/Exeter/dissertation/papers/labeled_faces_in_the_wild.pdf]]
    The database is aimed at studying the unseen pair match problem, differentiating any two individuals that have never been seen before :
    - no images of test subjects are available at training time
    - the decisions far all test pairs are made independently

    A training, validation and testing methodology has been provided
    by the authors.  Two views are used rather than a more traditional
    approach. The first view composed of 1100 pairs of matched and
    unmatched images for training and 500 pairs for testing can be
    used without restriction in order to compare as many models, with
    as many parameters settings as desired, and evaluate their
    performances. Once the best model has been retained, the second
    view is used to assess the accuracy on 10 separate experiments in
    a leave-one-out cross validation scheme. Estimated mean accuracy
    and standard error of the mean are then reported

    As far as the training phase is concerned, two different paradigms
    can be used : restricted and unrestricted settings. The former
    does not take the labelling of images into account in order to
    construct new pairs by transitivity. The latter classify the images
    by name, and thus training pairs can be constructed by selecting
    specific images from certain people. The unrestricted paradigm can
    be used if the algorithm developed could benefit from a more
    extensive training set.

*** Eigenfaces vs. Fisherfaces
    [[file:papers/eigenfaces_vs_fisherfaces.pdf][file:~/Exeter/dissertation/papers/eigenfaces_vs_fisherfaces.pdf]]
    
    In this paper, the problem of face recognition under large
    variations in lightning and facial expressions is studied. This is
    done by considering that the faces as Lambertian surface and thus
    being part of a 3D linear subspace of the high-dimensional image
    space.

    Fisherfaces, a supervised linear method based on a Linear
    Discriminant Analysis approach is used to build a model
    insensitive to such variations by maximising the ratio of
    between-class scatter to that of within-class scatter
    
    PCA can be used to reduce the dimensionality to an arbitrary
    number of features. However, LDA is limited by the number of
    classes present as the rank of the within-class scatter matrix is
    at most N - c where N is the number of images and c the number of
    classes when the number of samples N is less than the image
    dimensionality n. A solution to the small sample size problem is
    to first project the image set to a lower dimensional space in
    order to obtain a non singular matrix. This is achieved by
    reducing to N - c using PCA and then to c - 1 using LDA criterion.
    Additional techniques can be found in
    [[file:papers/small_sample_size_problem.pdf][file:~/Exeter/dissertation/papers/small_sample_size_problem.pdf]]

*** Distance Metric Learning
    [[file:papers/distance_metric_learning.pdf][file:~/Exeter/dissertation/papers/distance_metric_learning.pdf]]

    Unsupervised Distance Metric Learning or Manifold Learning

    Distance metric learning and dimensionality reduction techniques
    are closely linked. Every dimension reduction approach is
    essentially to learn a distance metric without label information.

** new papers
*** out-of-sample extensions

    Only parametric dimensionality reduction techniques, i.e.,
    techniques that learn an explicit function between the data space
    and the low-dimensional latent space, support exact out-of-sample
    extension. All linear techniques (PCA, LDA, NCA, MCML, LPP, and
    NPE) support exact out-of-sample extension, and autoencoders do
    too. Spectral techniques such as Isomap, LLE, and Laplacian
    Eigenmaps support out-of-sample extensions via the Nystr√∂m
    approximation.
    
    [[file:papers/out_of_sample_extensions.pdf][file:~/Exeter/dissertation/papers/out_of_sample_extensions.pdf]]
    Eigendecomposition-based dimension reduction methods providing an
    embedding for given training points have no straightforward
    extention for out-of-rample examples short of recomputing
    eigen-vectors. In this paper several of these unsupervised
    learning algorithms (LLE, Isomap, MDS, Eigenmaps, and Spectral
    Clustering) are extended by the use of a unified framework in
    which these algorithms are seen as learning eigenfunctions of a
    kernel.

    

*** Eigenproblems in pattern recognition
    [[file:papers/eigenproblems_in_pattern_recognition.pdf][file:~/Exeter/dissertation/papers/eigenproblems_in_pattern_recognition.pdf]]


* <2012-07-04 Wed>

** new papers
*** Algorithms for manifold learning
    [[file:papers/algorithms_for_manifold_learning.pdf][file:~/Exeter/dissertation/papers/algorithms_for_manifold_learning.pdf]]

    The dimensionality of the data set is artificially high, some
    features could even be irrelevant or even misleading. Algorithms,
    such as global optimisations, can be speed up by reducing the
    number of features. Manifold learning is based on the assumption
    that the data lies along a low-dimensional manifold embedded in a
    high-dimensional space.

    


* <2012-07-11 Wed>

** tasks 
*** TODO prepare presentation
*** TODO review mathematical part
*** TODO EigenFaces & Fisherfaces experiments
*** DONE review articles
    CLOSED: [2012-07-11 Wed 14:42]
*** TODO test framework

** review
*** The out-of-sample problem for CMDS
    CLOCK: [2012-07-11 Wed 13:19]--[2012-07-11 Wed 13:59] =>  0:40
    
    [[file:papers/out_of_sample_mds.pdf][file:~/Exeter/dissertation/papers/out_of_sample_mds.pdf]]

** presentation
*** plan
**** Dimensionality recognition for face recognition

* <2012-07-12 Thu>

** tasks
*** TODO write a paragraph about each methods
      - mathematical background
      - parameters
      - complexity
      - underlying hypotheses about the data


* <2012-07-17 Tue>
** Tasks
*** DONE Write a paragraph about each methods
    CLOSED: [2012-07-18 Wed 08:27]
    - introduction/goal of the method
    - instructions
    - mathematical explanation
    - underlying hypotheses
    - time and space complexity
    - graph/experiment (linear and non-linear)
    - comparison with other methods
    - references to specific papers
*** TODO write about the out-of-sample problem
*** DONE understanding LFW experimental methodology
    CLOSED: [2012-07-18 Wed 12:03]
*** DONE read Yiming source-code and add comments
    CLOSED: [2012-07-18 Wed 13:57]
*** TODO prepare simple experiments with the data
    - read the raw data and sift feature files
    - perform pca reduction and other reduction methods (think of applying LDA to the restricted setting)
    - represent the data as a graph
** experimental methodology
   The dataset is organised in two views. View 1 is for algorithm
   development and model selection. View 2 is for formal experiment
   and performance reporting

*** View 1
    The training set (pairsDevTrain.txt) consists of 1100 pairs of
    matched images and 1100 pairs of mismatched images. The test set
    (pairsDevTest.txt) consists of 500 pairs of matched and 500 of
    mismatched images.

*** View 2
    The second view of the data is composed of ten subsets of the
    database (pairs.txt). The performance of the classifier previously selected is
    evaluated on 10 separate experiments in a leave-one-out cross
    validation scheme, were nine of the subsets are successively
    combined using the 10th subset for testing.

*** Files Format
**** pairsDevTrain.txt
     1100 #number of matched pairs
     Aaron_Peirsol	1	2 #example of matched pair
     ... #rest of the matched pairs
     Wendy_Kennedy	1	Zara_Akhmadova	1 #example of dismatched pair
     ... #same number of matched pairs

**** pairsDevTest.txt     
     same format as pairsDevTrain.txt except that the number of
     matched pairs is 500.

**** pairs.txt
     10	300 #number of folds, number of matched and dismatched pairs per foldn
     Abel_Pacheco	1	4

**** SIFTPairF1.mat
     contains :
     -Data    10800x300 nSample x nFeature matrix   -   Training data
     -DD      2700x2    nPair x 2 matrix            -   Dissimilarity pairs  
     -SS      2700x2    nPair x 2 matrix            -   Similarity pairs
     -DataTT1 300x600   nFeature x nSample matrix   -   Testing data image 1 (1-300 positive 301-600 negative)
     -DataTT2 300x600   nFeature x nSample matrix   -   Testing data image 2

     Data 1:2700 first training data of positive class
     Data 27001:5400 second training data of positive class
     Data 5401:8100 first training data of negative class
     Data 8101:10800 second training data of negative class

     DataTT1 1:300 first testing data of positive class
     DataTT1 301:600 first testing data of negative class

     DataTT2 1:300 second testing data of positive class
     DataTT2 301:600 second testing data of negative class
     
     DD and SS is the combination of the 9 remaining subsets
     (300x9=2700) and denotes the indices of pairs to use to train the
     selected classifier relatively to the datapoints contained in
     Data.

** Matlab Code
*** CV_Train_Both_R_SIFT.m
    Train both DML and ITML for each of the SS and DD fold of the
    View2 of the data

    For each fold, the model optimal parameter selection is first
    performed via 3-cross validation, then the model is trained using
    this optimal parameter and saved.

    The datapoints features have already been reduced by PCA and only
    the first 35 features are used for training.

*** CV_Test_Both_R_SIFT.m
    

* <2012-07-20 Fri>
** Tasks
*** write script to obtain sift features
*** write script to build view1 and view2
*** test different reduction methods
*** evaluate time complexity
*** integrate with current programm
*** set up git repo
*** test out-of-sample extension
*** reflect upon LDA

** classification methodology
   Constitute a training and testing set. The training set will be
   constituted of pairs of similar and dissimilar images and therefore
   can be seperated into four distinct sets.

   Then a dimension reduction technique is performed based on the
   vectors of the training set. 

   The model is then trained using a distance metric learning
   approach. The distance between two vectors is computed and the
   classification threshold is defined such as maximising the accuracy
   for the training test. 

   Once all the parameters of the model are fixed, the out-of-sample
   extension of the dimension reduction technique is used to
   extrapolate the low-dimensional features of the testing set.

* <2012-07-23 Mon>
** experiment
   Run the script [[file:experiment/dimension_reduction.m][dimension_reduction.m]]
   dimension reduction 3456 features -> 300
   - PCA 290.28 seconds
   - ISOMAP 17270 seconds
   - LLE 482.26 seconds
   - Laplacian 1693.9 secondsx
* <2012-07-25 Wed>
  It is unrealistic to assume that the face manifold is well modeled
  by a linear subspace. It is also unlikely that the noise
  distribution is identical at each point in space.

* <2012-07-26 Thu>
** Tasks
*** TODO run [[file:experiment/yiming/verification_ml_test.m][verification_ml_test.m]] with Euclidean distance
*** TODO convert script to matlab
*** TODO synchronised linux cluster and local folders
*** TODO find command to run distant jobs
*** TODO compute dimensionality reductions
*** TODO trace accuracy/dimension and ROC diagrams
*** TODO implement LDA reduction

** Accuracy
   |          | SIFT-3456 | PCA-300 |
   | accuracy |   0.66800 | 0.67900 |
   |          |           |         |

** Observations
*** Non connected neighbourhood graphs
   Manifold learning techniques using a neighbourhood graph require a
   uniformaly distributed and well sampled data.

   It appears that the graph generated by LLE for the default
   parameters is not connected and therefore that some points are
   discarded while computing the low embedding.

   Solutions could be to generated a connected neighourhood graph
   ([[file:papers/building_connected_neighborhood_graphs_for_lle.PDF][building_connected_neighborhood_graphs_for_lle.PDF]]), find the
   right parameters, make use of the out-of-sample extension.

   This issue is reported in the drtoolbox FAQ:
   
     "Next to reducing the dimensionality of my data,
     Isomap/LLE/Laplacian Eigenmaps/LTSA also reduced the number of
     data points? Where did these points go?  You may observe this
     behavior in most techniques that are based on neighborhood
     graphs. Isomap/LLE/Laplacian Eigenmaps/LTSA can only embed data
     that gives rise to a connected neighborhood graph. If the
     neighborhood graph is not connected, the implementations only
     embed the largest connected component of the neighborhood
     graph. You can obtain the indices of the embedded data points
     from mapping.conn_comp (which you can get from the
     compute_mapping function). If you really need to have al your
     data points embedded, don‚Äôt use a manifold learner."
*** Reduction for different a different number of dimensions
    Can the reduction be computed once for all, and the desired number
    of features for the datavectors extrapolated from the result of
    the mapping?

    Dimensionality reduction techniques that make use of an
    eigen-decomposition relate the embedded dimension to the number of
    eigenvectors computed. Therefore by calculating all the
    eigenvectors, several dimensions of the embedding could be
    generated from a single eigen-analysis.
